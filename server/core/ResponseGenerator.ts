import { callOpenRouterChat } from "../adapters/OpenRouterAdapter";
import { limparResposta, formatarTextoEco } from "../utils/text";
import { hedge } from "../policies/hedge";

const MODEL_MAIN     = process.env.ECO_MODEL_MAIN     || "openai/gpt-5-chat";
const MODEL_TECH_ALT = process.env.ECO_MODEL_TECH_ALT || "openai/gpt-5-mini";

export async function fastGreet(prompt: string) {
  const lightSystem =
    "Voc√™ √© a ECO, acolhedora e concisa. Responda em 1‚Äì2 frases, em PT-BR, convidando a pessoa a come√ßar. Evite perguntas m√∫ltiplas.";
  const headers = {
    Authorization: `Bearer ${process.env.OPENROUTER_API_KEY!}`,
    "Content-Type": "application/json",
    "HTTP-Referer": process.env.PUBLIC_APP_URL || "http://localhost:5173",
    "X-Title": "Eco App - Fast Lane",
  };
  const data = await callOpenRouterChat(
    { model: MODEL_TECH_ALT, temperature: 0.6, max_tokens: 180, messages: [{ role: "system", content: lightSystem }, { role: "user", content: prompt }] },
    headers, 6000
  );
  const raw = data?.choices?.[0]?.message?.content ?? "Ol√°! üôÇ Como voc√™ quer come√ßar hoje?";
  return formatarTextoEco(limparResposta(raw));
}

export function microReflexoLocal(msg: string): string | null {
  const t = (msg || "").trim().toLowerCase();
  if (/cansad/.test(t)) return "Entendi. Parece que o corpo est√° pedindo pausa. Quer come√ßar com 1 minuto de respira√ß√£o ou prefere s√≥ desabafar um pouco?";
  if (/ansios/.test(t)) return "Percebo ansiedade a√≠. Topa notar 3 pontos de apoio do corpo agora e, se quiser, me contar onde ela pega mais?";
  if (/triste/.test(t)) return "Sinto a tristeza chegando. Prefere nomear o que mais doeu ou que eu guie uma micro-pausa?";
  if (/irritad|raiva/.test(t)) return "Raiva √© energia. Quer soltar em palavras o gatilho principal, sem filtro, ou tentamos baixar um pouco a ativa√ß√£o primeiro?";
  if (/medo|receio|insegur/.test(t)) return "Tem medo no ar. Podemos mapear rapidamente: 1) o que amea√ßa, 2) o que te protege, 3) qual seria o pr√≥ximo passo menor. Topa?";
  return null;
}

export async function chatCompletion(messages: any[], maxTokens: number) {
  const headers = {
    Authorization: `Bearer ${process.env.OPENROUTER_API_KEY!}`,
    "Content-Type": "application/json",
    "HTTP-Referer": process.env.PUBLIC_APP_URL || "http://localhost:5173",
    "X-Title": "Eco App - Chat",
  };
  const main = callOpenRouterChat(
    { model: MODEL_MAIN, messages, temperature: 0.7, top_p: 0.9, presence_penalty: 0.1, frequency_penalty: 0.1, max_tokens: maxTokens },
    headers, 9000
  );
  const mini = callOpenRouterChat(
    { model: MODEL_TECH_ALT, messages, temperature: 0.65, top_p: 0.9, max_tokens: Math.min(420, maxTokens) },
    headers, 5500
  );
  return hedge(main, mini, 2500);
}
